## Feature learning

[Iteratively reweighted kernel machines efficiently learn sparse functions](https://arxiv.org/abs/2505.08277)   
L. Zhu,&nbsp;D. Davis,&nbsp;D. Drusvyatskiy,&nbsp;and&nbsp;M. Fazel.&nbsp;Preprinted.

[Emergence in non-neural models: grokking modular arithmetic via average gradient outer product](https://arxiv.org/abs/2407.20199)  
N. Mallinar,&nbsp;D. Beaglehole,&nbsp;L. Zhu,&nbsp;A. Radhakrishnan,&nbsp;P. Pandit,&nbsp;and&nbsp;M. Belkin.&nbsp;ICML&nbsp;2025 (Oral).

[Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning](https://arxiv.org/abs/2306.04815)  
L. Zhu,&nbsp;C. Liu,&nbsp;A. Radhakrishnan,&nbsp;and&nbsp;M. Belkin.&nbsp;ICML&nbsp;2024.

[Quadratic models for understanding catapult dynamics of neural networks](https://arxiv.org/abs/2205.11787)  
L. Zhu,&nbsp;C. Liu,&nbsp;A. Radhakrishnan,&nbsp;and&nbsp;M. Belkin.&nbsp;ICLR&nbsp;2024.

[A note on Linear Bottleneck networks and their Transition to Multilinearity](https://arxiv.org/abs/2206.15058)  
L. Zhu,&nbsp;P. Pandit,&nbsp;and&nbsp;M. Belkin.&nbsp;Preprinted.

## Kernel regime

[Assembly and iteration: transition to linearity of wide neural networks](https://www.sciencedirect.com/science/article/abs/pii/S1063520325000880)  
C. Liu,&nbsp;L. Zhu,&nbsp;and&nbsp;M. Belkin.&nbsp;Applied and Computational Harmonic Analysis (ACHA)&nbsp; 2025.

[Neural tangent kernel at initialization: linear width suffices](https://proceedings.mlr.press/v216/banerjee23a/banerjee23a.pdf)  
A. Banerjee,&nbsp;P. Cisneros-Velarde,&nbsp;L. Zhu,&nbsp;and&nbsp;M. Belkin.&nbsp;UAI&nbsp;2023.

[Restricted Strong Convexity of Deep Learning Models with Smooth Activations](https://arxiv.org/abs/2209.15106)  
A. Banerjee,&nbsp;P. Cisneros-Velarde,&nbsp;L. Zhu,&nbsp;and&nbsp;M. Belkin.&nbsp;ICLR&nbsp;2023.

[Transition to Linearity of General Neural Networks with Directed Acyclic Graph Architecture](https://arxiv.org/abs/2205.11786)  
L. Zhu,&nbsp;C. Liu,&nbsp;and&nbsp;M. Belkin.&nbsp;NeurIPS&nbsp;2022.

[Loss landscapes and optimization in over-parameterized non-linear systems and neural networks](https://arxiv.org/abs/2003.00307)  
C. Liu,&nbsp;L. Zhu,&nbsp;and&nbsp;M. Belkin.&nbsp;Applied and Computational Harmonic Analysis (ACHA)&nbsp; 2022.
  
[Transition to Linearity of Wide Neural Networks is an Emerging Property of Assembling Weak Models](https://arxiv.org/abs/2203.05104)  
C. Liu,&nbsp;L. Zhu,&nbsp;and&nbsp;M. Belkin.&nbsp;ICLR&nbsp;2021&nbsp;(Spotlight).

[On the linearity of large non-linear models: when and why the tangent kernel is constant](https://arxiv.org/abs/2010.01092)  
C. Liu,&nbsp;L. Zhu,&nbsp;and&nbsp;M. Belkin.&nbsp;NeurIPS&nbsp;2020&nbsp;(Spotlight).


## Random matrix theory

[Spectral norm bound for the product of random Fourier-Walsh matrices](https://arxiv.org/abs/2504.03148)  
L. Zhu,&nbsp;D. Davis,&nbsp;D. Drusvyatskiy,&nbsp;and&nbsp;M. Fazel.&nbsp;Preprinted.